---
title: "Homework 2"
execute:
  error: true
author: "Norah Deckert"
output: html_document
---

## Reading In the Data

First, read in the CSV data of cookie ingredients.
Make sure that your end-result data has appropriate types for each column - these should match the types provided in the documentation in the README.md file.

```{r}
library(skimr)
library(readr)
choc_chip_cookie_ingredients <- read_csv("choc_chip_cookie_ingredients.csv")
skim(choc_chip_cookie_ingredients)
skim(choc_chip_cookie_ingredients, Recipe_Index, Rating)

#try to write a code to skim to find how many recipes were rated with a 0.8 or higher
skim(choc_chip_cookie_ingredients, numeric(length(0.8)))
Rating = function(choc_chip_cookie_ingredients, numeric)
skim(choc_chip_cookie_ingredients, numeric = Rating(mean))
skim(choc_chip_cookie_ingredients) |> print()

#Using the skimr package, I was able to find that about 50% of the ratings had an 87% rating of their cookie recipe or higher. I also found that the average cookie recipe rating was an 81.5%. The output included the console, a skim of numeric values, and a skim of categories. Furthermore, it showed histograms for the data and any missing data based on the parameters I put inside the skim.

```

cookies <- read.csv("choc_chip_cookie_ingredients.csv")
cookies |> str()
cookies |> filter(Recipe_Index == "AR_1")
cookies |> group_by(Ingredient) |>
  summarize(avg_rating = mean(Rating, na.rm=T), n=n(), missing_ratings = sum(is.na(Rating)))|>
  arrange(desc(avg_rating)) |>
  ggplot(aes(x = n - missing_ratings, y = avg_rating, label = Ingredient)) + geom_point()
ggplotly()

library(reticulate)
py_install(c("skimpy", "numpy"))

```{python}
import pandas as pd
df = pd.read_csv("choc_chip_cookie_ingredients.csv")
print(df.head())
print(df.info())
dtype_map = {
  "Ingredient": "string",
  "Text": "string",
  "Recipe_Index": "string",
  "Rating": "float64",
  "Quantity":"float64",
  "Unit":"float64"
}
for col, dt in dtype_map.items():
  if col in df.columns:
    df[col] = df[col].astype(dt)
print(df.info())
print(df.describe(include = "all"))

count_high = (df["Rating"] >= 0.8).sum()
total_non_missing = df["Rating"].notna().sum()
prop_high = count_high / total_non_missing
avg_rating = df["Rating"].mean()

```


## Exploratory Data Analysis

Exploratory data analysis is the process of getting familiar with your dataset. To get started, [this blog post](https://www.mrdbourke.com/a-gentle-introduction-to-exploratory-data-analysis/) provides a nice checklist to get you thinking:

> 1.  What question(s) are you trying to solve (or prove wrong)?
> 2.  What kind of data do you have and how do you treat different types?
> 3.  What's missing from the data and how do you deal with it?
> 4.  Where are the outliers and why should you care about them?
> 5.  How can you add, change or remove features to get more out of your data?

### Generating Questions

Generate at least 5 questions you might explore using this database of cookie ingredients.

1. How many different recipes can be randomly generated using every ingredient?
2. Which chocolate chip recipe has the highest overall rating?
3. Which chocolate chip recipe uses the most ingredients overall and how does that affect the Rating?
4. Does the difference in Recipe_Index correlate with the overall rating of the Recipe?
5. Using the range from 0.0 to 0.1, what ingredients of chocolate chip recipes creates the highest rated chocolate chip recipe?
6. How many chocolate chip recipes are missing ratings and how would this affect finding the best recipe?

### Skimming the Data

One thing we often want to do during EDA is to examine the quality of the data - are there missing values? What quirks might exist in the dataset?

The `skimr` package in R, and the similar `skimpy` package in python (which has a much better name, in my opinion), can help provide visual summaries of the data. 

Install both packages, and read the package documentation ([R](https://cran.r-project.org/web/packages/skimr/vignettes/skimr.html), [Python](https://pypi.org/project/skimpy/)).

[Part 1] Use each package and generate summaries of your data that require the use of at least some non-default options in each package's `skim` function.


```{r}
library(skimr)
library(readr)
library(dplyr)

choc_chip_cookie_ingredients <- read_csv(
  "choc_chip_cookie_ingredients.csv",
  col_types = cols(Ingredient = col_character(), Text = col_character(), Recipe_Index = col_character(), Rating = col_double(), Quantity = col_double(), Unit = col_double()))

skim(choc_chip_cookie_ingredients)
skim(choc_chip_cookie_ingredients, Rating, Quantity)

choc_chip_cookie_ingredients %>%
  select(where(is.numeric)) %>%
  skim() 

```

```{python}
import pandas as pd
from skimpy import skim
import matplotlib.pyplot as plt

df = pd.read_csv("choc_chip_cookie_ingredients.csv")
dtype_map {
   "Ingredient": "string",
  "Text": "string",
  "Recipe_Index": "string",
  "Rating": "float64",
  "Quantity":"float64",
  "Unit":"float64"
}
for col, dt in dtype_map.items():
  if col in df.columns:
    df[col] = df[col].astype(dt)
    
skim(df)
skim(df.select_dtypes(includes = "number"))


```

[Part 2] Write 1-2 sentences about what you can tell from each summary display you generate. Did you discover anything new about the data?

#Using the skimpy and skimr packages, I am able to generate summary statistics and identify missing data in the chocolate chip cookie recipe dataset. The numeric summary showed that the Rating values were closer to the higher end of the scale, but Quantity is pretty right skewed. This shows that the recipes in this dataset are mostly well-rated, but the Quantity of ingredients varies.

### Generating Tables

Another useful technique for exploratory data analysis is to generate summary tables. 
You may want to use the `dplyr` package in R (`group_by` or `count` functions), as well as the `groupby` and `count` methods in Pandas. [Python example](https://sparkbyexamples.com/pandas/pandas-groupby-count-examples/), [R example](https://dplyr.tidyverse.org/reference/count.html)

[Part 1] Using R and Python, generate a table that shows what **proportion** of recipes contain each type of ingredient, for the most common 20 ingredients.

```{r}
#make a table of the top 20 ingredients
library(dplyr)
library(readr)
df <- read_csv("choc_chip_cookie_ingredients.csv")
#total unique recipes
total_recipes <- n_distinct(df$Recipe_Index)

ingredient_props <- df %>%
  distinct(Recipe_Index, Ingredient) %>%
  count(Ingredient, name = "recipe_count") %>%
  mutate(proportion = recipe_count / total_recipes) %>%
  arrange(desc(proportion))

top20_ingredients <- ingredient_props %>%
  slice_head(n = 20)
print(top20_ingredients)

```

[Part 2] Print out a character string that lists all of the ingredients that do not appear in at least 20 recipes.

```{r}
#ingredients appearing in < 20 recipes
rare_ingredients <- ingredient_props %>%
  filter(recipe_count < 20) %>%
  pull(Ingredient)

rare_string <- paste(rare_ingredients, collapse = ", ")
cat(rare_string)


```



### Visualization

Using whatever plotting system you are comfortable with in R or python, see if you can create a couple of useful exploratory data visualizations which address one of the questions you wrote above - or another question which you've come up with as you've worked on this assignment.

[Part 1] Create at least one plot (it doesn't have to be pretty) that showcases an interesting facet of the data.

[Part 2] Write 2-3 sentences about what you can learn from that plot and what directions you might want to investigate from here.

#
#make sure to also find the 3 ingredients that tie for last
